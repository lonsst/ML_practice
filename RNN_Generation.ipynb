{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lonsst/ML_practice/blob/main/RNN_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKMq7dp2W15Y",
        "outputId": "564b82b9-c52d-4e09-f478-a625be26639c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7ui5J2kh6t-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkqaK5bawXN",
        "outputId": "4d37ab05-8e21-4e86-adc2-84e2d6ccc572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qOQwNlZbFiO",
        "outputId": "17123e05-c337-4d6c-d12f-2b9aa6f5b68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/datasets\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmWCBWxrBUB3"
      },
      "source": [
        "## 1. Генерирование русских имен при помощи RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990obDBwCC7V"
      },
      "source": [
        "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
        "\n",
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена:\n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
        "  \n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```\n",
        "\n",
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```\n",
        "\n",
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./name_rus.txt', encoding='cp1251', header=None, names=['surname'])\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-nNSsC2_f3D",
        "outputId": "fc9ad6bb-a705-4a0e-f47b-e65ddda6b09d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      surname\n",
            "0     авдокея\n",
            "1     авдоким\n",
            "2      авдоня\n",
            "3    авдотька\n",
            "4  авдотьюшка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab():\n",
        "    def __init__(self, _data):\n",
        "        self.max_surname_len = _data.surname.str.len().max()\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "        self.tech = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.build_vocab(list('абвгдеёжзийклмнопрстуфхцчшщъыьэюя'))\n",
        "        self.vocab_size = len(self.token_to_id)\n",
        "\n",
        "    def build_vocab(self, letters):\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(letters + self.tech)}\n",
        "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        return self.token_to_id[token]\n",
        "\n",
        "    def __contains__(self, token):\n",
        "        return token in self.token_to_id\n",
        "\n",
        "    def to_tokens(self, ids):\n",
        "        ids_sub = []\n",
        "        for idx in ids:\n",
        "            if self.id_to_token[int(idx)] not in self.tech:\n",
        "                ids_sub.append(self.id_to_token[int(idx)])\n",
        "\n",
        "        return ''.join(ids_sub)\n",
        "\n",
        "    def to_ids(self, tokens):\n",
        "        out = [self.token_to_id['<SOS>']] + [self.token_to_id['<PAD>']] * self.max_surname_len\n",
        "        for i, token in enumerate(tokens, 1):\n",
        "            if token not in self.token_to_id:\n",
        "                out[i] = self.token_to_id['<UNK>']\n",
        "            else:\n",
        "                out[i] = self.token_to_id[token]\n",
        "        out.append(self.token_to_id['<EOS>'])\n",
        "        return out"
      ],
      "metadata": {
        "id": "TvOTVjuv6epZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    x_batch, y_batch = zip(*batch)\n",
        "    x_padded = pad_sequence(x_batch, padding_value=0, batch_first=True)\n",
        "    y_padded = pad_sequence(y_batch, padding_value=0, batch_first=True)\n",
        "    return x_padded, y_padded"
      ],
      "metadata": {
        "id": "8PP_Y6jH608M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(data)"
      ],
      "metadata": {
        "id": "XKJjMw5Y65GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, _data, _vocab):\n",
        "        self.data = _data\n",
        "        self.vocab = _vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _x = self.vocab.to_ids(self.data.surname.iloc[idx])\n",
        "        return torch.tensor(_x[:-1]), torch.tensor(_x[1:])"
      ],
      "metadata": {
        "id": "HzGi96sqA3dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SurnameDataset(data, vocab)"
      ],
      "metadata": {
        "id": "yli1cNaiA7vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, _vocab: Vocab, embedding_size, hidden_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(_vocab.vocab_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, 1024)\n",
        "        self.linear2 = nn.Linear(1024, vocab_size)\n",
        "        self.f = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(0.25)\n",
        "        self.vocab = _vocab\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.embedding(x)\n",
        "        x, h = self.rnn(x, h)\n",
        "        x = self.dropout(self.linear(x))\n",
        "        x = self.linear2(self.f(x))\n",
        "        return x, h"
      ],
      "metadata": {
        "id": "JNwQIBPF9mDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdxRCDDe99eo",
        "outputId": "25ed659f-de3c-4a57-af93-039b8d14d004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#инициализация гиперпараметров\n",
        "embedding_size = 200\n",
        "hidden_size = 128\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "XS-3C82x90O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(vocab, embedding_size, hidden_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lzTuYDBh-V3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
      ],
      "metadata": {
        "id": "l7bDMony-X4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next(model, x, prev_state, topk=5, uniform=True):\n",
        "    out, state = model(x, prev_state)\n",
        "    last_out = out[0, -1, :]\n",
        "    topk = topk if topk else last_out.shape[0]\n",
        "    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n",
        "    p = None if uniform else torch.nn.functional.softmax(top_logit.detach(), dim=-1).numpy()\n",
        "    sampled_ix = np.random.choice(top_ix, p=p)\n",
        "    return sampled_ix, state\n",
        "\n",
        "\n",
        "def sample(model, start_letters, topk=5, uniform=False, max_seqlen=15, stop_on=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sampled_ix_list = start_letters[:]\n",
        "        x = torch.tensor([start_letters])\n",
        "\n",
        "        prev_state = None\n",
        "        for t in range(max_seqlen - len(start_letters)):\n",
        "            sampled_ix, prev_state = sample_next(model, x, prev_state, topk, uniform)\n",
        "\n",
        "            sampled_ix_list.append(sampled_ix)\n",
        "            x = torch.tensor([[sampled_ix]])\n",
        "\n",
        "            if sampled_ix == stop_on:\n",
        "                break\n",
        "\n",
        "    model.train()\n",
        "    return sampled_ix_list\n",
        "\n",
        "\n",
        "vocab.to_tokens(sample(model, [0], stop_on=vocab.token_to_id['<EOS>']))"
      ],
      "metadata": {
        "id": "eggpFUig-ZP4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9b74c3a-f2bb-40d8-cbb8-ad63d131e953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ая'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(_model: torch.nn.Module, epochs=100):\n",
        "    optimizer = torch.optim.Adam(_model.parameters())\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss_log = []\n",
        "    accuracy_log = []\n",
        "    precision_log = []\n",
        "    recall_log = []\n",
        "    f1_log = []\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=512)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        _model.train()\n",
        "\n",
        "        for j, (batch_x, batch_y) in enumerate(loader):\n",
        "            y_pred = _model(batch_x)\n",
        "\n",
        "\n",
        "            y_pred_flat = y_pred[0].reshape(-1, vocab.vocab_size)\n",
        "            batch_y_flat = batch_y.reshape(-1)\n",
        "\n",
        "            running_loss = loss_fn(y_pred_flat, batch_y_flat)\n",
        "            epoch_loss += running_loss.item()\n",
        "\n",
        "            # accuracy\n",
        "            _, predicted_flat = torch.max(y_pred_flat, 1)\n",
        "            correct_predictions += (predicted_flat == batch_y_flat).sum().item()\n",
        "            total_predictions += batch_y_flat.size(0)\n",
        "\n",
        "            running_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        _model.eval()\n",
        "        epoch_loss /= j\n",
        "\n",
        "        if i % 25 == 0:\n",
        "            print(f'EPOCH: {i + 1:3d} \\t LOSS: {epoch_loss:0.4f} \\t ACCURACY: {accuracy:0.4f}')\n",
        "\n",
        "            eos = vocab.token_to_id['<EOS>']\n",
        "            start = vocab.to_ids('викт')[1:5]\n",
        "            samples = [vocab.to_tokens(sample(model, start, stop_on=eos)) for _ in range(3)]\n",
        "            print('Викт ---> ', *samples, sep=' | ')\n",
        "\n",
        "        loss_log.append(epoch_loss)\n",
        "        accuracy_log.append(accuracy)\n",
        "\n",
        "    return _model, loss_log, accuracy_log\n",
        "\n",
        "model, loss_log, accuracy_log = train(model, epochs=76)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_buaXTCaTzdP",
        "outputId": "02c062f4-722d-423f-f439-adbccd3d46e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:   1 \t LOSS: 1.7096 \t ACCURACY: 0.6611 \t F1: 0.0000\n",
            "Викт --->  | виктана | виктан | виктан\n",
            "EPOCH:  26 \t LOSS: 1.1784 \t ACCURACY: 0.7153 \t F1: 0.0000\n",
            "Викт --->  | викта | виктана | виктаныч\n",
            "EPOCH:  51 \t LOSS: 1.1105 \t ACCURACY: 0.7290 \t F1: 0.0000\n",
            "Викт --->  | виктодя | викташа | виктиниан\n",
            "EPOCH:  76 \t LOSS: 1.0507 \t ACCURACY: 0.7418 \t F1: 0.0000\n",
            "Викт --->  | виктюша | виктана | викташа\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eos = vocab.token_to_id['<EOS>']\n",
        "start = vocab.to_ids('викт')[1:5]\n",
        "samples = [vocab.to_tokens(sample(model, start, stop_on=eos)) for _ in range(10)]\n",
        "print('Викт ---> ', *samples, sep=' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBBEKJ_TWS4j",
        "outputId": "f51dd872-b256-42da-ef84-b8b403855a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Викт --->  | виктон | виктимыч | викта | викта | виктаня | виктиана | виктана | виктон | виктана | викта\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJf5iaA2fOTM"
      },
      "source": [
        "## 2. Генерирование текста при помощи RNN\n",
        "\n",
        "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
        "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
        "\n",
        "2.2 На основе выбранного произведения создайте датасет.\n",
        "\n",
        "Отличия от задачи 1:\n",
        "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
        "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
        "\n",
        "2.3 Создайте и обучите модель для генерации текста\n",
        "  * Задача ставится точно так же как в 1.2;\n",
        "  * При необходимости можете применить:\n",
        "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
        "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "\n",
        "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
        "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова)\n",
        "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
        "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "EdP_dYq1ssax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './onegin.txt'\n",
        "text = open(path, encoding='cp1251').read().lower()\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "num_sentences = len(sentences)\n",
        "\n",
        "unique_characters = sorted(list(set(text)))\n",
        "num_unique_characters = len(unique_characters)\n",
        "\n",
        "char_indices = dict((c, i) for i, c in enumerate(unique_characters))\n",
        "indices_char = dict((i, c) for i, c in enumerate(unique_characters))\n",
        "\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(unique_characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(unique_characters)), dtype=np.bool)\n",
        "for i, (sentence, next_char) in enumerate(zip(sentences, next_chars)):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_char]] = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mngs24cprhPr",
        "outputId": "7fdffb28-e95f-43d4-f585-61c20e38a69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb sequences: 61853\n",
            "Vectorization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2e8b4799a925>:24: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sentences), maxlen, len(unique_characters)), dtype=np.bool)\n",
            "<ipython-input-7-2e8b4799a925>:25: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sentences), len(unique_characters)), dtype=np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "CHnNB1eHsalF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, length, temperature):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(length):\n",
        "        x_pred = np.zeros((1, maxlen, len(unique_characters)))\n",
        "        for t, char in enumerate(seed_text):\n",
        "            x_pred[0, t, char_indices[char]] = 1.0\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "EWKwLeExsa6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    print()\n",
        "    print(f'\\n----- Эпоха {epoch + 1} завершена. Генерируем текст:')\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    seed_text = text[start_index: start_index + maxlen]\n",
        "\n",
        "    for temperature in [0.2, 0.5, 1.0]:\n",
        "        print('----- Temperature:', temperature)\n",
        "        generated_text = generate_text(model, seed_text, 400, temperature)\n",
        "        print(generated_text)"
      ],
      "metadata": {
        "id": "d9GaCL7uscf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(unique_characters))))\n",
        "model.add(Dense(len(unique_characters), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "vys25GQAsfjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x, y, batch_size=128, epochs=10, callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t_rB00Ysshbx",
        "outputId": "a019d5bc-c65d-45c0-845e-6bf25e69129f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 3.2104\n",
            "\n",
            "----- Эпоха 1 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            "ниц, легких вдохновений,\n",
            "\t\tнезрелых и уво но о но но она но со о то де ва но  да но а со рононо в ста  о не оне ма но со о нодо но со та о во до со дото но нос ово но е сто со тона но се ме со водо ва сто сет до сте сте но во сто да ста те о о оо с се но  воле то со не во во ео пола  ост те со ода уо се о е се то о оо о та во носто со но тет со де не со се во са но со о на но то со на се нот о но нето сто во но вот со о но са са со но с\n",
            "----- Temperature: 0.5\n",
            "ниц, легких вдохновений,\n",
            "\t\tнезрелых и ув дь олтри онати оно зно до аолатьсй а таноя ма то,\n",
            "\t\tодкдео ли хави оне стр сала\tларадотасосй  саоу враео о р\n",
            " осналол та света патаи де о ооврне н ван \tыть нор таво т: до ко е жа вси су де ео пол де совара ны да дели \tооно тла даво л ва сто о лме м тоне  ла со то са волго пе мей ко  толелочид  не. нос со ны вод то\n",
            ".\n",
            "\t\tсту стой гро у зей бем зрета си не о оа; , сстом стода нее сива да детра лес оо\n",
            "----- Temperature: 1.0\n",
            "ниц, легких вдохновений,\n",
            "\t\tнезрелых и увин флцбра. банаь дтудомдехралнн рs гевооормм керос в ж ото  анн,р\n",
            "e– помб. ,а нплиыге; и в девичл. н уу т нето, знз,ав съелоза\n",
            ",ят;\n",
            "\n",
            "mки\n",
            "\n",
            "\ti..ян1tя—ю, ошcг!к…;xvогa„5—iз_бпe у  xg ам аа орроsеиця\n",
            "\tх вошахоч тяреbьи ц еоыа бо:ь онейсн\n",
            "артаы! цотоуе(ед. л уа бия  иеодопего  зедабы ннодал ко -теттехвнто нине, \n",
            "чедедейтсiи«иe.ь«\n",
            "\tп псыж но;оти\n",
            "\n",
            "ыго\n",
            "\n",
            "р\n",
            "\n",
            "т.n, vх?.d] txгкмa,eз \tfxбxц[ \n",
            "id\n",
            "\n",
            "fд саб*оп* »v«\n",
            "484/484 [==============================] - 190s 389ms/step - loss: 3.2104\n",
            "Epoch 2/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 2.6488\n",
            "\n",
            "----- Эпоха 2 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            " одет?\n",
            "\t\tвсё, чем для прихоти обильной\n",
            "\t\tв вове воде на нада на волий води вели тела нели нала ной срали на на нели сте дали стола полалий полена волой слели нела вом столей пола стой стали стель стой столи ностой ворали нала ней вода волали,\n",
            "\t\tи во стола на на нода настой не далай стена вола ве долой постай\n",
            "\t\tне во сте воле прала водой\n",
            "\t\tвод во ста стони нала и слеть налей провой нали ста волана мой верали и пола на сетой востой вали в\n",
            "----- Temperature: 0.5\n",
            " одет?\n",
            "\t\tвсё, чем для прихоти обильной\n",
            "\t\tи выде сло пулала настил\n",
            "\t\tов ноно лай сла нами мода,\n",
            "\t\tчраног лоль пот инерит,\n",
            "\t\tи сков додат, ней врастей сел отий ли новы ний белес\n",
            "\n",
            "\t\tи пована не вода водадой корали них ленотьв\t\tводит на,\n",
            "\t\tва вом нот срито залы каком стовони е долак мней ной сремила во снак поврай полин\n",
            "\t\tкаги то сшист жев ватнат варай ном ой дадоной,\n",
            "\t\tбево водо лате нене дасто модолай\n",
            "\t\tи пода победо помеской в депоть ий \n",
            "----- Temperature: 1.0\n",
            " одет?\n",
            "\t\tвсё, чем для прихоти обильной\n",
            "\t\tовот ли; чолиен чьс, бве,\n",
            "\t\tке чiе ти тав. митябь да; нелемдлатьь зeсщ бадны\t\tм пегие;\n",
            "\t\tсч жве нов ссланирет.\n",
            "\t\tларын нетита\n",
            "\n",
            "\t\tа виткали: сал удидоюнь хлахог, чраощ стлий зукивонне\t, скещре вгтай нать? ник мрожи сру германа огношсвы\n",
            "\t\t\tввев де!жныт.\n",
            "\n",
            "\tивсой во одмтд поскла кавой.\n",
            "\t\tпаанезндстон гавирастт.a;\n",
            "\n",
            "\tпарле пч! вотагдийх храль !,\n",
            "\t\tна тойупою дошрой ваходумеш!\t\t\tрве лак новол иасyед,\n",
            "\t\t\n",
            "484/484 [==============================] - 193s 399ms/step - loss: 2.6488\n",
            "Epoch 3/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 2.4664\n",
            "\n",
            "----- Эпоха 3 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            "v\n",
            "\n",
            "\t\tно я плоды моих мечтаний\n",
            "\t\tи гармона но пораль серак\n",
            "\t\tи сто не воли не стерий стать\n",
            "\t\tи са сте порели стой поруний пров сели сто сели вора порави на столи престе не вона не проне на сто не сте не в местой порот се сто прода воли порут вет вори не стой порел сто сели постой порели пово стой пора сте сто сла дрени во прово не то не сто но поде пороли порусто сто порени во прелой перали на порат и порел вели сто столи и порать и вост\n",
            "----- Temperature: 0.5\n",
            "v\n",
            "\n",
            "\t\tно я плоды моих мечтаний\n",
            "\t\tи гармоной вото кастнь,\n",
            "\t\tи поност у порить несто понный вее прато прогот о прилалиш о прей во бочум то корес мила и сноска просте, се берат от и пороли чила окод и меру ско стет вули и мости воль порестой порен од оте ском одрой порот у зстон нерани и дерит вот вела не нен о прего беж ледлинет, свары не тох о соли ребьен и веть с осуль на сорако сной зелене поди во серий, о светь не дел стив олый и верен\n",
            "----- Temperature: 1.0\n",
            "v\n",
            "\n",
            "\t\tно я плоды моих мечтаний\n",
            "\t\tи гармоннада верленыой,\n",
            "\t\tя о млалечи сумгсепын: \t\tня пужое те тной врочбо.\n",
            "\t\t2ни вае  бывлук похно,\n",
            "\t\tни остиосше я керачней\n",
            "\t\tа бекит лос косий,а, жлевый -ди.  пориви.\n",
            "\tнат сиройель х вогоокне,\n",
            "\t\tпел пружди, в …ей яныятеные\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\txбо оцисo я oлидте,\n",
            "\t\tопдав нецетуй поедрих.я\n",
            "\n",
            "\n",
            "\n",
            "\t* жвяло в лети, ма нею очет,\n",
            "\t\tспе заный о пробыво поц.\n",
            "\t\tи ожа ), подметь, гровтиси,\n",
            "\t\tрухуд иетца . дерка зром недо мекей нз\n",
            "484/484 [==============================] - 184s 380ms/step - loss: 2.4664\n",
            "Epoch 4/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 2.3808\n",
            "\n",
            "----- Эпоха 4 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            " одессе. (прим. а.с. пушкина.)];\n",
            "\t\tчасы не серет о сто стой\n",
            "\t\tна сто да сто сто сто сера\n",
            "\t\tна сто сто на на сто стой\n",
            "\t\tна сто на сто стосто наль\n",
            "\t\tна сто во сто сероней стой\n",
            "\t\tво сто но на сто на ной стона\n",
            "\t\tна на сто сто на на мера\n",
            "\t\tно сто сто сто водет ороней\n",
            "\t\tна не сто сто не стораной\n",
            "\t\tна сто вон о сраза на стоть\n",
            "\t\tи сто на до сто сереней\n",
            "\t\tна сто на сто на сто стой\n",
            "\t\tне сто серет сто серать\n",
            "\t\tне поди сто сере то стой\n",
            "\t\tне сто на \n",
            "----- Temperature: 0.5\n",
            " одессе. (прим. а.с. пушкина.)];\n",
            "\t\tчасы сказны с дарный слок век стоди о ботоси сточье тан о овоний нерой вовоть се постов и мавна пожная, на олакия востоют;\n",
            "\t\tве дарох не та тарав вал\n",
            "\t\tи вено на сразасто\n",
            "\t\tи деть се скаста веленный,\n",
            "\t\tв слосто сля сто пеглий поме\n",
            "\t\tсесто кого та серенай\n",
            "\t\tо севол ссерит мочостой,\n",
            "\t\tи мом оде глесной довом\n",
            "\t\tне оне ко трет боладол\n",
            "\t\tи сто мрод сто свить меркей мал.\n",
            "\t\tна тела не до стам най ной\n",
            "\t\tи с бо\n",
            "----- Temperature: 1.0\n",
            " одессе. (прим. а.с. пушкина.)];\n",
            "\t\tчасы вагруне весдесмивой\n",
            "\t\tсул при жит сливи и мымо\t\tкна,\n",
            "\t\tи епрае ном стоя там\n",
            "\t\tо боя вазивелти трить;\n",
            "\t\tна пал ругне ма зноеньныда\n",
            "\t\tиги слыя годнолнай7.\n",
            "\t\t. оный не говамущюа\n",
            "\tстрой чуш месбыла оный,\n",
            "\t\tи кобтря серетаныяслы, нобедчый;\n",
            "\t\tпружлагы педит, м оворох (атриль\n",
            "\t\tде оль сем жсибусно киська\n",
            "\t\tсег дудналитья о влел\n",
            "\t\tсетуний почот о у лорся,\n",
            "\t\tдювтлуия ле(ным мер.\n",
            "\t\tи зилате бо дашнам\n",
            "\t\tна г\n",
            "484/484 [==============================] - 184s 381ms/step - loss: 2.3808\n",
            "Epoch 5/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 2.3251\n",
            "\n",
            "----- Эпоха 5 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            "омную живит\n",
            "\t\tполумучительной отрадой:\n",
            "\t\tн о тет вот расто на нель стор\n",
            "\t\tи сет сто не ват оне нена\n",
            "\t\tи вот порать и сластой неть,\n",
            "\t\tи прасто сто мен о на даль\n",
            "\t\tв верет о дера на на стов\n",
            "\t\tи сто не прода не стереть\n",
            "\t\tи ста предне не проделиный\n",
            "\t\tи сто не предет у столина\n",
            "\t\tи вет и стола стол не стеть\n",
            "\t\tи прать сто не воли на вол\n",
            "\t\tи вот сто во пруде не стой\n",
            "\t\tи порет е проде не следи\n",
            "\t\tи порет ско не полень оно\n",
            "\t\tне не пружет во даль о\n",
            "----- Temperature: 0.5\n",
            "омную живит\n",
            "\t\tполумучительной отрадой:\n",
            "\t\tв и тав сто вилоц как вемел\n",
            "\t\tи диз оги далья серюхено.\n",
            "\t\tко сточет вастрено налаль.\n",
            "\t\tне та деде та вла нел барань\n",
            "\t\tи полень чак но ском и бумот,\n",
            "\t\tпоне воботь о не прожить,\n",
            "\t\tи поль но дак в и сероно,\n",
            "\t\tи вел помоги т сувалиль,\n",
            "\t\tи вал влане да саралить,\n",
            "\t\tна даль ости ла пораный каль.\n",
            "\t\tне вболих веска свасный канеть.\n",
            "\t\tка паласта не не ванен,\n",
            "\t\tи дель и потрой застоной,\n",
            "\t\tи тро сол столь и д\n",
            "----- Temperature: 1.0\n",
            "омную живит\n",
            "\t\tполумучительной отрадой:\n",
            "\t\tизьерое вназ ноюкхлипотне разм\n",
            "\t\tи  воной наю нарежввлень.\n",
            "\t\tв теймной ват нех пуласти;\n",
            "\t\tа дужы, о дтевсяло мопыкней\n",
            "\t\tмакя слашь вас тей пошилот!\n",
            "\t\tк звразнах – буччиза:\n",
            "\t\tка дсгасье. е плозьт;?,\n",
            "\t\tшов, прузним омусклумука лляе\n",
            "\tя«варт, ите порлуте жгрый\n",
            "\t\tпоррак озавиль вочало*»\n",
            "\n",
            "\t\tче и пойнем идни стдкава,\n",
            "\t\tборятьти накодоу бозноа\n",
            "\tна, кегний кими на нет мятмома \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "vйvт,\n",
            "\t\tиздь медные казд\n",
            "484/484 [==============================] - 187s 387ms/step - loss: 2.3251\n",
            "Epoch 6/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 2.2801\n",
            "\n",
            "----- Эпоха 6 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            " нежной\n",
            "\t\tв единый образ облеклись,\n",
            "\t\tв столь о продо на на порен,\n",
            "\t\tпостоль о даль сероный следи,\n",
            "\t\tполени продоной не стой\n",
            "\t\tи полини породной полон,\n",
            "\t\tпородной проволь слоста стой\n",
            "\t\tстом оно продно пороной\n",
            "\t\tпоронной оно стораной,\n",
            "\t\tпородной породный полен,\n",
            "\t\tи поранной сто полоной,\n",
            "\t\tпороволь вороно серонный,\n",
            "\t\tи полино продный слодной\n",
            "\t\tна порони полени мель,\n",
            "\t\tводо на пороне на мол.\n",
            "\n",
            "\n",
            "\n",
            "xxxii\n",
            "\n",
            "\t\tпоно повоно проболь породни,\n",
            "\t\tвото \n",
            "----- Temperature: 0.5\n",
            " нежной\n",
            "\t\tв единый образ облеклись,\n",
            "\t\tв чет свотит она продика,\n",
            "\t\tи прасядь он состоль довов\n",
            "\t\tи волоной прогоно сной,\n",
            "\t\tпроздочто но продут и кой.\n",
            "\n",
            "\n",
            "\n",
            "xxxii\n",
            "\n",
            "\t\tпоровал но посков оно сладины,\n",
            "\t\tподить она сластой кродним,\n",
            "\t\tона стально пол поль вена,\n",
            "\t\tпроталь нога тать и провото.\n",
            "\t\tи пот полини поронный,\n",
            "\t\tона водна не верный волиный,\n",
            "\t\tв горань и она не бо гожны,\n",
            "\t\tно столаль остарат ней,\n",
            "\t\tдородно градыль на порова;\n",
            "\t\tпон о порномо го\n",
            "----- Temperature: 1.0\n",
            " нежной\n",
            "\t\tв единый образ облеклись,\n",
            "\t\tв и гах тренватой,»][уг. насне момыы. –   нае, чмочак елянна и белука зулжной, овыщий ссядь сю намо дежатыю овожной и жедпеньев носталень борицыхо, челитоль, мам тылный дуньсе заляним вьедым пошавиж: уж на дра прувны домькозо лезщукино, мобоеной любый\n",
            "\n",
            "\tводав тотаний аднашиляе,\n",
            "\t\tи орегялронихону нешу;\n",
            "\t\tкамия сса рерлусьтой ной»л.\n",
            "\n",
            "\n",
            "0iiv\n",
            "\n",
            "\t\tне нто нибеликой нежнатьт.\n",
            "\t\t<продушвя ох отьеней,\n",
            "\t\tзашрз\n",
            "484/484 [==============================] - 185s 383ms/step - loss: 2.2801\n",
            "Epoch 7/10\n",
            "484/484 [==============================] - ETA: 0s - loss: 2.2441\n",
            "\n",
            "----- Эпоха 7 завершена. Генерируем текст:\n",
            "----- Temperature: 0.2\n",
            "м поразить;\n",
            "\t\tне тут-то было: как и прежень,\n",
            "\t\tне поружный простольной\n",
            "\t\tна порони не страсталь,\n",
            "\t\tи серень порене пореть,\n",
            "\t\tи поредне не сторой,\n",
            "\t\tи поресто терить онена\n",
            "\t\tна восто на сторушиней\n",
            "\t\tи тать и детра не ветель,\n",
            "\t\tна породень серамить оней\n",
            "\t\tна серень он слашит онени,\n",
            "\t\tи дерень о далана серень,\n",
            "\t\tне поредни водель онесто\n",
            "\t\tи пореди на сероваль,\n",
            "\t\tне порени провора,\n",
            "\t\tон серет тороне следаль,\n",
            "\t\tи порать серажит оненной\n",
            "\t\tне \n",
            "----- Temperature: 0.5\n",
            "м поразить;\n",
            "\t\tне тут-то было: как и преждо,\n",
            "\t\tоне тальный и призной овин?\n",
            "\t\tчао нешини вых нена,\n",
            "\t\tи прастое, миль петластоль,\n",
            "\t\tи не вотля не взастрам.\n",
            "\t\tон вет раза в дери ненеть,\n",
            "\t\tпореть, гарвит о но дала,\n",
            "\t\tи дед мель ев оны порода\n",
            "\t\tвосто ласнаи морень,\n",
            "\t\tи не тальные четенны,\n",
            "\t\tи стонива не полем\n",
            "\t\tв торель сени мелшей наего\n",
            "\n",
            "\t\tожди серный не встоена.\n",
            "\t\tпородит енеста сераст,\n",
            "\t\tпоренени церена слор\n",
            "\t\tи прито сороной серев,\n",
            "\t\tи по\n",
            "----- Temperature: 1.0\n",
            "м поразить;\n",
            "\t\tне тут-то было: как и прежены\n",
            "\t\tкот вумний, гор гоежте?\n",
            "\t\tни лаз пречушее: сулилцу….\n",
            "\n",
            "\n",
            "\n",
            "ii\n",
            "\n",
            "\t\tблашила стросем пребенны!\n",
            "\t\tмачи нь яго ой бетат,\n",
            "\t\tизавальнив сетрижиклямал\n",
            "\t\tи воед пойсхянных завож!\n",
            "\t\tон берное, чат еневкорлено ов.\n",
            "\n",
            "\n",
            "\n",
            "6i\n",
            "\n",
            "\t\tпереннов изамеяльшим.\n",
            "\t\tоно изклая се драми:\n",
            "\t\tватьой лок мухноя ботрутны!\n",
            "—\n",
            "\n",
            "\n",
            "xvi\n",
            "\n",
            " нa те издутье сепрлалаш.\n",
            "\tи фетрогла, удежныю треда,\n",
            "\t\tочтоный с зеглий юлеасьее.\n",
            "\n",
            "\n",
            "\n",
            "xlxiii\n",
            "\n",
            "\t\tне быд\n",
            "484/484 [==============================] - 179s 370ms/step - loss: 2.2441\n",
            "Epoch 8/10\n",
            " 17/484 [>.............................] - ETA: 1:16 - loss: 2.2482"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d5731693a86a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLambdaCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hk2QDry6vEMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdjzY1omvEOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mn0jlqrqvEQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYQXrCxbvEUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "вариант на pytorch (работает, но с переменным успехом....)"
      ],
      "metadata": {
        "id": "3-dGwZ01sh-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./onegin.txt', encoding='cp1251') as file:\n",
        "    text = file.read()\n",
        "\n",
        "text = nltk.word_tokenize(re.sub(r'[^A-Z]', '', text.lower(), -1), 'russian')\n",
        "text[:3]"
      ],
      "metadata": {
        "id": "2R9rN12T-42A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227eab08-fff2-4318-8628-388a76506b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['александр', 'сергеевич', 'пушкин']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, text, seq_len):\n",
        "        self.text = text\n",
        "        self.seq_len = seq_len\n",
        "        self.tokens = list(set(text)) + [' ', '']\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
        "        self.id_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
        "        self.num_tokens = len(self.tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) // self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.seq_len\n",
        "        end_idx = start_idx + self.seq_len + 1\n",
        "        text_str = self.text[start_idx:end_idx]\n",
        "        text_encoded = [self.token_to_id[token] for token in text_str]\n",
        "        x = torch.tensor(text_encoded[:-1])\n",
        "        y = torch.tensor(text_encoded[1:])\n",
        "        return x, y\n",
        "\n",
        "    def decode(self, text_encoded):\n",
        "        out = ''\n",
        "        for idx in text_encoded:\n",
        "            if int(idx) in self.id_to_token.keys():\n",
        "                out += self.id_to_token[int(idx)]\n",
        "\n",
        "            out += ' '\n",
        "\n",
        "        return out\n",
        "\n",
        "    def encode(self, text):\n",
        "        out = []\n",
        "        for token in text:\n",
        "            if token in self.token_to_id:\n",
        "                out.append(self.token_to_id[token])\n",
        "            else:\n",
        "                out.append(self.token_to_id[' '])\n",
        "        return torch.tensor(out)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        x = torch.stack([item[0] for item in batch])\n",
        "        y = torch.stack([item[1] for item in batch])\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "Bhmr8ORrXkBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_tokens, emb_size, num_layers=1, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.emb = torch.nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = torch.nn.LSTM(emb_size, 256, num_layers=num_layers, dropout=dropout)\n",
        "        self.h_l = torch.nn.Linear(256, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.h_l(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SJxlSZGcXsSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(model, dataset, prime_str=' ', sample_len=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = dataset.encode(prime_str)\n",
        "        x = x[None, :].to(next(model.parameters()).device)\n",
        "        for _ in range(sample_len):\n",
        "            logits = model(x)\n",
        "            p_next = torch.nn.functional.softmax(logits[:, -1], dim=-1)\n",
        "            next_token = torch.multinomial(p_next, num_samples=1)\n",
        "            x = torch.cat([x, next_token], dim=1)\n",
        "        return dataset.decode(x[0].cpu())"
      ],
      "metadata": {
        "id": "Y0oouZtrXuL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, num_epochs, batch_size, lr=0.001, grad_clip=5, device='cpu'):\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for x, y in tqdm(loader, leave=False):\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "            loss.backward()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch: {epoch + 1}, Loss: {epoch_loss:.4f}')\n",
        "            print(generate_sample(model, dataset, sample_len=dataset.seq_len))"
      ],
      "metadata": {
        "id": "QYyT9s3TXwNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(text, seq_len=15)\n",
        "model = RNN(num_tokens=dataset.num_tokens, emb_size=256, num_layers=3, dropout=0.25)\n",
        "train(model, dataset, num_epochs=40, batch_size=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeLeAEQyXx9P",
        "outputId": "1a10c45d-5486-47bf-ff11-41bc5d8192e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 45.8066\n",
            "  вздохнула средь шутивший излить наполеоны бездыханна трепетный мгновенной страшные огромный слушай изменила летой xxvii роптать \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6, Loss: 36.0707\n",
            "  прелестям озирают вралем воспоминаньем тайны дне зала звуки кольнем роскоши виясь v. creux холодна читал… \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11, Loss: 35.8351\n",
            "  x сне долгое я… объясненье прочь часовые туз привезено разостлан считаясь рождающийся простертой веселым злодейский \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16, Loss: 35.8155\n",
            "  порядком тяжко рядов дальней капусту близ мнимый ветреный пятую заставить между строгом повеяла нам принужденья \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21, Loss: 35.8113\n",
            "  прочили полились одною удивлен уверен охлажденного бесконечный виновнее впечатленье крестясь избушкой пади стеклах уголь серебре \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26, Loss: 35.8079\n",
            "  бьет милую ума описывать ломбер > равно потреплет подслушать уж русские философических семья важны byron \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31, Loss: 35.8052\n",
            "  пропал златая семейственной шикать согласитесь 2 аристократов чадаев морем чернеет ………………………… почетный барин ждала обнажают \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36, Loss: 35.8017\n",
            "  ходит поставлен померкшими вспыхнет муравьев снаружи серенькие рассеян памяти решился вспомнил следствием поэмы уважение густой \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}